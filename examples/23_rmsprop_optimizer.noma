// Example 23: RMSprop Optimizer
// Demonstrates RMSprop (Root Mean Square Propagation) optimizer

fn main() {
    // Select RMSprop optimizer (optimizer = 3.0)
    let optimizer = 3.0;       // 1=SGD, 2=Adam, 3=RMSprop
    
    // RMSprop hyperparameters
    let learning_rate = 0.01;
    let beta2 = 0.9;           // Squared gradient decay (default 0.9)
    let epsilon = 0.00000001;  // Numerical stability (1e-8)
    let max_iterations = 2000;
    
    // Rosenbrock function optimization
    // f(x,y) = (1-x)^2 + 100*(y-x^2)^2
    // Minimum at (1, 1)
    learn x = -1.0;
    learn y = 1.0;
    
    optimize(x) until loss < 0.001 {
        let a = 1.0 - x;
        let b = y - x * x;
        let loss = a * a + 100.0 * b * b;
        minimize loss;
    }
    
    print(x);  // Should converge to ~1.0
    print(y);  // Should converge to ~1.0
    return x;
}
