// XOR benchmark version - fixed iterations for fair comparison
// Forces exactly 200 + 120 = 320 steps (same as NumPy/C++)

fn main() {
    // XOR dataset
    let X = tensor [
        [0.0, 0.0],
        [0.0, 1.0],
        [1.0, 0.0],
        [1.0, 1.0]
    ];
    let Y = tensor [
        [0.0],
        [1.0],
        [1.0],
        [0.0]
    ];

    // Initial parameters (small model: hidden=2)
    learn W1 = rand_normal_tensor(0.0, 0.5, 2.0, 2.0);
    learn W2 = rand_normal_tensor(0.0, 0.5, 2.0, 1.0);

    // Phase 1: exactly 200 iterations (no early stop for fair comparison)
    let optimizer = 2.0;      // Adam
    let learning_rate = 0.05;
    let max_iterations = 200.0;

    optimize(W1) until loss < 0.0 {  // Never true, forces full 200 iters
        let hidden = sigmoid(matmul(X, W1));
        let pred = sigmoid(matmul(hidden, W2));
        let err = pred - Y;
        let loss = mean(err * err);
        print(loss);
        minimize loss;
    }

    // Growth: widen the hidden layer
    realloc W1 = [2.0, 16.0];
    realloc W2 = [16.0, 1.0];

    // Phase 2: exactly 120 more iterations (total 320, matching NumPy/C++)
    let optimizer = 2.0;      // Adam
    let learning_rate = 0.12;
    let max_iterations = 120.0;

    optimize(W1) until loss < 0.0 {  // Never true, forces full 120 iters
        let hidden = sigmoid(matmul(X, W1));
        let pred = sigmoid(matmul(hidden, W2));
        let err = pred - Y;
        let loss = mean(err * err);
        print(loss);
        minimize loss;
    }

    return loss;
}
